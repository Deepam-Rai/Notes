
# Neural Turing Machines (NTM)
 > **Turing Complete:** Given proper configuration the model can solve any computable problem.
 
> **Working memory:** Transient system of storing and manipulation of information.

NTM tries to replicate Von-Neuman architecture of computers.

Architectural components of NTM:
1. Controller
	- CPU counterpart.
	- Controller learns its program while CPU is fed its program.
	- Usually RNNs are used.
		- Augmenting External memory to RNNs prunes large portion of its search space. RNNs are turing complete.
1. External Memory
	- RAM counterpart.
2. Read heads
	- Memory Read Buses counterpart.
	- Can be multiple.
1. Write heads
	- Memory Write Buses counterpart.
	- Can be multiple.

> **End-to-end differentiable:** Architecture where gradients can be computed for output loss and based upon these gradients the model's parameter, that process the input, can be updated.
- NTM architecture needs to be end-to-end differentiable.
	- $\implies$ The memory(which is discrete) should be accessible in continuous manner.

# Attention Based Memory Access
- Allows to access discrete memory in continuous manner.
- Can be used in end-to-end differentiable architecture.


$N =$ Number of memory locations,  
$W =$ Size of a memory location,  
$t =$ time step,  
$M_t =$ Memory matrix of dim $(N \times W)$ at time-step $t$,  

### For read head
Generate a softmax normalized attention/weighting vector that tells how much attention to give to each location.

$w_t =$ attention vector of size $N$ for time step $t$, generated by controller,  
$r_t =$ read vector at time step $t$, of dim $W$.
$$\large r_t = M_t^Tw_t$$
- We look at all $N$ memory location at once.
- $w_t$ tells us how much attention to give to each location.
- We don't pin-point any location but give blurry focus to all locations.

### For write head
Generate weighting vector that tells which locations to focus erasing and writing.  

$w_t =$ weighting vector of dim $N$ with values in $[0,1]$, generated by controller,  
$e_t =$ erase vector of dim $W$, generated by controller,  
$\circ =$ element wise multiplication,  
$v_t =$ write vector of dim $W$, generated by controller,  
$E =$ matrix of ones, of dim $(N \times W)$.
$$\large M_t = M_{t-1} \circ \left( E - w_te_t^T \right) + w_tv_t^T$$
- $w_t$ component for $i'th$ memory location is small $\approx 0\implies w_te_t^T$ component is small $\implies 1-(small)$ is close to 1 $\implies$ last memory content of $M_{t-1} \times 1$ is approximately remains same.  
- $w_t$ is large $\rightarrow$ erase that point in first term and write on that point in second term.

# NTM Memory Addressing Mechanisms
- How weightings vectors are generated.
- What form of addressing do they represent.

## Content Based Addressing
Fetch from locations whose contents are similar to key vector generated by controller.  

$k_t =$ key vector generated by controller,  
$Sim() =$ some similarity measure,  
$\beta =$ parameter key strength,  
$$\Large \begin{aligned}
w(M[j],k,\beta) &= \text{weight of j'th location} \\
&= \frac{\exp \left[ \beta Sim(M[j],k) \right]}{\sum_{i=0}^N{ \exp\left[ \beta Sim(M[i],k) \right]}}
\end{aligned}$$

- It is normalized softmax distribution over similarity scores.
- **$\beta$:** When similarity ~uniform for all memory locations attention weights also becomes similar. $\beta$ attenuates the weight such that the difference in weights becomes significant and attention can be focused on few locations only.
	- e.g., To give intuition: $[0.0011, 0.0059, 0.0099]$ difference in weights is small. for $\beta =100$ weights become $[0.11, 0.59, 0.99]$ the difference now increases by several decimals. In actual computation softmax is calculated.

 **Interpolation gating:** $g_t$ controls whether to focus on current location or the addressing given by content based lookup.  
 $g_t =$ interpolation gate,  
 $w^c_t =$ Content based addressing,  
  $w_{t-1} =$ current location,  
  $w^g_t =$ gated weighting,  
  $$\large w^g_t =g_tw^c_t +(1-g_t)w_{t-1}$$

- $g_t =0 \implies$ more weight to current location. Helps in consecutive memory accessing.
- $g_t =1 \implies$ more weight to content based addressing.

 **shift weighting:** Shifts the focus from one location to another using current gated weighting.
 - $s_t =$ normalized attention vector of size n+1.
 - It applies convolution on the gated weighting.
 - Instead of applying padding at the ends it uses rotational convolutional operator.